{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58517613",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import nltk\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f721a282",
   "metadata": {},
   "outputs": [],
   "source": [
    "german_stop_words = stopwords.words('german')\n",
    "german_stop_words.append(\"fur\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b14e8a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONSTANTS\n",
    "DATA_PATH = \"D:/10kgerdataset/\"\n",
    "TRAIN_CSV = \"train.csv\"\n",
    "TEST_CSV = \"test.csv\"\n",
    "CLASS_TO_IDX = {\n",
    "    \"etat\": 0,\n",
    "    \"inland\": 1,\n",
    "    \"international\": 2,\n",
    "    \"kultur\": 3,\n",
    "    \"panorama\": 4,\n",
    "    \"sport\": 5,\n",
    "    \"web\": 6,\n",
    "    \"wirtschaft\": 7,\n",
    "    \"wissenschaft\": 8\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59b623ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df_train = pd.read_csv(os.path.join(DATA_PATH, TRAIN_CSV))\n",
    "    df_test = pd.read_csv(os.path.join(DATA_PATH, TEST_CSV))\n",
    "except FileNotFoundError:\n",
    "    print(\"File was not found at specific location.\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da7b7187",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(document: str) -> str:\n",
    "    return re.sub(r'[^\\w\\s]', '', document)\n",
    "\n",
    "def remove_numbers(document: str) -> str:\n",
    "    return re.sub(r'$\\d+\\W+|\\b\\d+\\b|\\W+\\d+$', '', document)\n",
    "\n",
    "def map_umlaut(document: str) -> str:\n",
    "    umlaut_mapping = {\n",
    "        \"ß\": \"b\",\n",
    "        \"ü\": \"u\",\n",
    "        \"ä\": \"a\",\n",
    "        \"ö\": \"o\",\n",
    "        \"ë\": \"e\",\n",
    "    }\n",
    "    for k, v in umlaut_mapping.items():\n",
    "        document = document.replace(k, v)\n",
    "    return document\n",
    "\n",
    "def stop_word_removal(document: str) -> str:\n",
    "    return \" \".join(w for w in document.split() if w not in german_stop_words)\n",
    "\n",
    "def save_vocab(vocab, path):\n",
    "    output = open(path, 'wb')\n",
    "    pickle.dump(vocab, output)\n",
    "    output.close()\n",
    "\n",
    "def load_vocab(path):\n",
    "    output = open(path, 'rb')\n",
    "    vocabulary = pickle.load(output)\n",
    "    output.close\n",
    "    return vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21e59a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = load_vocab(\"vocabulary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3f39557",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pre_processing_pipeline(df, tokenize: bool):\n",
    "    new_df = df.copy(deep=False)\n",
    "    \n",
    "    new_df[\"text\"] = new_df[\"text\"].str.lower()\n",
    "    new_df[\"label\"] = new_df[\"label\"].str.lower()\n",
    "    \n",
    "    new_df = new_df.dropna()\n",
    "    \n",
    "    new_df[\"text\"] = new_df[\"text\"].apply(remove_punctuation)\n",
    "    new_df[\"text\"] = new_df[\"text\"].apply(remove_numbers)\n",
    "    new_df[\"text\"] = new_df[\"text\"].apply(map_umlaut)\n",
    "    new_df[\"text\"] = new_df[\"text\"].apply(stop_word_removal)\n",
    "    \n",
    "    if tokenize:\n",
    "        new_df[\"text\"] = new_df[\"text\"].apply(lambda x: x.split())\n",
    "        new_df[\"text\"] = new_df[\"text\"].apply(lambda x: vocab(x))\n",
    "        new_df[\"label\"] = new_df[\"label\"].apply(lambda x: CLASS_TO_IDX[x])\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13447cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GnadDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        try:\n",
    "            data = run_pre_processing_pipeline(df, True)\n",
    "            self.x = np.array(data[\"text\"])\n",
    "            self.y = torch.stack([torch.tensor(label) for label in data[\"label\"]])\n",
    "        except Exception:\n",
    "            raise\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.x.shape[0]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "baaf84a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = GnadDataset(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb1a0807",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_wrapper(batch):\n",
    "    batch = list(zip(*batch))\n",
    "    max_length = max(len(arr) for arr in batch[0])\n",
    "    \n",
    "    inp = torch.stack([F.pad(torch.tensor(arr), (0, max_length - len(arr))) for arr in batch[0]])\n",
    "    tgt = torch.stack(batch[1])\n",
    "    seq_lengths = torch.stack([torch.tensor(len(arr)) for arr in batch[0]])\n",
    "    return inp, tgt, seq_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9602a087",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_wrapper)\n",
    "\n",
    "# for batch_ndx, (data, target, seq_lengths) in enumerate(train_dataloader):\n",
    "#     a = data\n",
    "#     b = target\n",
    "#     c = seq_lengths\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "568b0ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, vocab_size, e_dim, h_dim, num_layer, output_dim, is_bidirectional: bool = True):\n",
    "        super().__init__()\n",
    "        self.h_dim = h_dim\n",
    "        self.directions = 2 if is_bidirectional else 1\n",
    "        self.embedding_layer = nn.Embedding(num_embeddings=vocab_size, embedding_dim=e_dim, padding_idx=0)\n",
    "        self.gru_layer = nn.LSTM(input_size=e_dim,\n",
    "                                hidden_size=h_dim,\n",
    "                                num_layers=num_layer,\n",
    "                                batch_first=True,\n",
    "                                bidirectional=True)\n",
    "        self.fc_1 = nn.Linear(h_dim * 2, output_dim)\n",
    "        \n",
    "    def forward(self, x, seq_lengths):\n",
    "        x = self.embedding_layer(x)\n",
    "        x = nn.utils.rnn.pack_padded_sequence(input=x, lengths=seq_lengths, batch_first=True, enforce_sorted=False)\n",
    "        _, (h_state, c_state) = self.gru_layer(x)\n",
    "        \n",
    "        # The outputs of the two directions of the LSTM are concatenated on the last dimension\n",
    "        x = torch.cat((h_state[-2, :, :], h_state[-1, :, :]), dim=1)\n",
    "        x = self.fc_1(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "75c49b80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "model = Model(vocab_size=len(vocab), e_dim=64, h_dim=128, num_layer=4, output_dim=10)\n",
    "model.train()\n",
    "model.to('cuda')\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-2)\n",
    "num_epoch = 20\n",
    "def train_model():\n",
    "    for epoch in range(num_epoch):  # loop over the dataset multiple times\n",
    "        running_loss = 0.0\n",
    "        for i, (data, labels, seq_lengths) in enumerate(train_dataloader):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            data = data.to(device=device)\n",
    "            labels = labels.to(device=device)\n",
    "            \n",
    "\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(data, seq_lengths)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            if i % 20 == 0:    # print every 200 mini-batches\n",
    "                print('[%d, %5d] loss: %.3f' %\n",
    "                      (epoch + 1, i + 1, running_loss / 2000))\n",
    "                running_loss = 0.0\n",
    "\n",
    "        print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "adfe1e25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,     1] loss: 0.001\n",
      "[1,    21] loss: 0.023\n",
      "[1,    41] loss: 0.023\n",
      "[1,    61] loss: 0.022\n",
      "[1,    81] loss: 0.022\n",
      "[1,   101] loss: 0.021\n",
      "[1,   121] loss: 0.021\n",
      "[1,   141] loss: 0.021\n",
      "[1,   161] loss: 0.021\n",
      "[1,   181] loss: 0.021\n",
      "[1,   201] loss: 0.021\n",
      "[1,   221] loss: 0.021\n",
      "[1,   241] loss: 0.021\n",
      "Finished Training\n",
      "[2,     1] loss: 0.001\n",
      "[2,    21] loss: 0.021\n",
      "[2,    41] loss: 0.021\n",
      "[2,    61] loss: 0.020\n",
      "[2,    81] loss: 0.021\n",
      "[2,   101] loss: 0.020\n",
      "[2,   121] loss: 0.021\n",
      "[2,   141] loss: 0.020\n",
      "[2,   161] loss: 0.021\n",
      "[2,   181] loss: 0.020\n",
      "[2,   201] loss: 0.020\n",
      "[2,   221] loss: 0.020\n",
      "[2,   241] loss: 0.020\n",
      "Finished Training\n",
      "[3,     1] loss: 0.001\n",
      "[3,    21] loss: 0.020\n",
      "[3,    41] loss: 0.020\n",
      "[3,    61] loss: 0.020\n",
      "[3,    81] loss: 0.020\n",
      "[3,   101] loss: 0.019\n",
      "[3,   121] loss: 0.019\n",
      "[3,   141] loss: 0.020\n",
      "[3,   161] loss: 0.019\n",
      "[3,   181] loss: 0.019\n",
      "[3,   201] loss: 0.019\n",
      "[3,   221] loss: 0.019\n",
      "[3,   241] loss: 0.019\n",
      "Finished Training\n",
      "[4,     1] loss: 0.001\n",
      "[4,    21] loss: 0.019\n",
      "[4,    41] loss: 0.019\n",
      "[4,    61] loss: 0.019\n",
      "[4,    81] loss: 0.019\n",
      "[4,   101] loss: 0.019\n",
      "[4,   121] loss: 0.019\n",
      "[4,   141] loss: 0.019\n",
      "[4,   161] loss: 0.018\n",
      "[4,   181] loss: 0.018\n",
      "[4,   201] loss: 0.019\n",
      "[4,   221] loss: 0.019\n",
      "[4,   241] loss: 0.018\n",
      "Finished Training\n",
      "[5,     1] loss: 0.001\n",
      "[5,    21] loss: 0.019\n",
      "[5,    41] loss: 0.017\n",
      "[5,    61] loss: 0.018\n",
      "[5,    81] loss: 0.018\n",
      "[5,   101] loss: 0.019\n",
      "[5,   121] loss: 0.019\n",
      "[5,   141] loss: 0.019\n",
      "[5,   161] loss: 0.018\n",
      "[5,   181] loss: 0.018\n",
      "[5,   201] loss: 0.018\n",
      "[5,   221] loss: 0.017\n",
      "[5,   241] loss: 0.018\n",
      "Finished Training\n",
      "[6,     1] loss: 0.001\n",
      "[6,    21] loss: 0.017\n",
      "[6,    41] loss: 0.017\n",
      "[6,    61] loss: 0.017\n",
      "[6,    81] loss: 0.017\n",
      "[6,   101] loss: 0.018\n",
      "[6,   121] loss: 0.017\n",
      "[6,   141] loss: 0.017\n",
      "[6,   161] loss: 0.017\n",
      "[6,   181] loss: 0.016\n",
      "[6,   201] loss: 0.017\n",
      "[6,   221] loss: 0.018\n",
      "[6,   241] loss: 0.017\n",
      "Finished Training\n",
      "[7,     1] loss: 0.001\n",
      "[7,    21] loss: 0.016\n",
      "[7,    41] loss: 0.016\n",
      "[7,    61] loss: 0.017\n",
      "[7,    81] loss: 0.017\n",
      "[7,   101] loss: 0.017\n",
      "[7,   121] loss: 0.016\n",
      "[7,   141] loss: 0.016\n",
      "[7,   161] loss: 0.016\n",
      "[7,   181] loss: 0.016\n",
      "[7,   201] loss: 0.016\n",
      "[7,   221] loss: 0.016\n",
      "[7,   241] loss: 0.016\n",
      "Finished Training\n",
      "[8,     1] loss: 0.001\n",
      "[8,    21] loss: 0.016\n",
      "[8,    41] loss: 0.016\n",
      "[8,    61] loss: 0.017\n",
      "[8,    81] loss: 0.016\n",
      "[8,   101] loss: 0.016\n",
      "[8,   121] loss: 0.016\n",
      "[8,   141] loss: 0.016\n",
      "[8,   161] loss: 0.016\n",
      "[8,   181] loss: 0.016\n",
      "[8,   201] loss: 0.016\n",
      "[8,   221] loss: 0.016\n",
      "[8,   241] loss: 0.015\n",
      "Finished Training\n",
      "[9,     1] loss: 0.001\n",
      "[9,    21] loss: 0.015\n",
      "[9,    41] loss: 0.015\n",
      "[9,    61] loss: 0.016\n",
      "[9,    81] loss: 0.015\n",
      "[9,   101] loss: 0.015\n",
      "[9,   121] loss: 0.015\n",
      "[9,   141] loss: 0.014\n",
      "[9,   161] loss: 0.015\n",
      "[9,   181] loss: 0.014\n",
      "[9,   201] loss: 0.015\n",
      "[9,   221] loss: 0.016\n",
      "[9,   241] loss: 0.015\n",
      "Finished Training\n",
      "[10,     1] loss: 0.001\n",
      "[10,    21] loss: 0.014\n",
      "[10,    41] loss: 0.014\n",
      "[10,    61] loss: 0.014\n",
      "[10,    81] loss: 0.014\n",
      "[10,   101] loss: 0.014\n",
      "[10,   121] loss: 0.015\n",
      "[10,   141] loss: 0.015\n",
      "[10,   161] loss: 0.017\n",
      "[10,   181] loss: 0.015\n",
      "[10,   201] loss: 0.014\n",
      "[10,   221] loss: 0.015\n",
      "[10,   241] loss: 0.015\n",
      "Finished Training\n",
      "[11,     1] loss: 0.001\n",
      "[11,    21] loss: 0.014\n",
      "[11,    41] loss: 0.014\n",
      "[11,    61] loss: 0.014\n",
      "[11,    81] loss: 0.013\n",
      "[11,   101] loss: 0.014\n",
      "[11,   121] loss: 0.014\n",
      "[11,   141] loss: 0.014\n",
      "[11,   161] loss: 0.015\n",
      "[11,   181] loss: 0.014\n",
      "[11,   201] loss: 0.014\n",
      "[11,   221] loss: 0.013\n",
      "[11,   241] loss: 0.014\n",
      "Finished Training\n",
      "[12,     1] loss: 0.001\n",
      "[12,    21] loss: 0.013\n",
      "[12,    41] loss: 0.013\n",
      "[12,    61] loss: 0.013\n",
      "[12,    81] loss: 0.013\n",
      "[12,   101] loss: 0.014\n",
      "[12,   121] loss: 0.013\n",
      "[12,   141] loss: 0.014\n",
      "[12,   161] loss: 0.013\n",
      "[12,   181] loss: 0.013\n",
      "[12,   201] loss: 0.015\n",
      "[12,   221] loss: 0.013\n",
      "[12,   241] loss: 0.013\n",
      "Finished Training\n",
      "[13,     1] loss: 0.001\n",
      "[13,    21] loss: 0.013\n",
      "[13,    41] loss: 0.013\n",
      "[13,    61] loss: 0.012\n",
      "[13,    81] loss: 0.012\n",
      "[13,   101] loss: 0.013\n",
      "[13,   121] loss: 0.013\n",
      "[13,   141] loss: 0.012\n",
      "[13,   161] loss: 0.013\n",
      "[13,   181] loss: 0.013\n",
      "[13,   201] loss: 0.013\n",
      "[13,   221] loss: 0.013\n",
      "[13,   241] loss: 0.012\n",
      "Finished Training\n",
      "[14,     1] loss: 0.001\n",
      "[14,    21] loss: 0.012\n",
      "[14,    41] loss: 0.012\n",
      "[14,    61] loss: 0.012\n",
      "[14,    81] loss: 0.012\n",
      "[14,   101] loss: 0.012\n",
      "[14,   121] loss: 0.011\n",
      "[14,   141] loss: 0.012\n",
      "[14,   161] loss: 0.012\n",
      "[14,   181] loss: 0.012\n",
      "[14,   201] loss: 0.012\n",
      "[14,   221] loss: 0.013\n",
      "[14,   241] loss: 0.012\n",
      "Finished Training\n",
      "[15,     1] loss: 0.000\n",
      "[15,    21] loss: 0.012\n",
      "[15,    41] loss: 0.011\n",
      "[15,    61] loss: 0.011\n",
      "[15,    81] loss: 0.012\n",
      "[15,   101] loss: 0.011\n",
      "[15,   121] loss: 0.012\n",
      "[15,   141] loss: 0.011\n",
      "[15,   161] loss: 0.011\n",
      "[15,   181] loss: 0.012\n",
      "[15,   201] loss: 0.011\n",
      "[15,   221] loss: 0.012\n",
      "[15,   241] loss: 0.012\n",
      "Finished Training\n",
      "[16,     1] loss: 0.001\n",
      "[16,    21] loss: 0.010\n",
      "[16,    41] loss: 0.010\n",
      "[16,    61] loss: 0.011\n",
      "[16,    81] loss: 0.011\n",
      "[16,   101] loss: 0.010\n",
      "[16,   121] loss: 0.010\n",
      "[16,   141] loss: 0.011\n",
      "[16,   161] loss: 0.010\n",
      "[16,   181] loss: 0.010\n",
      "[16,   201] loss: 0.011\n",
      "[16,   221] loss: 0.012\n",
      "[16,   241] loss: 0.011\n",
      "Finished Training\n",
      "[17,     1] loss: 0.001\n",
      "[17,    21] loss: 0.011\n",
      "[17,    41] loss: 0.010\n",
      "[17,    61] loss: 0.010\n",
      "[17,    81] loss: 0.011\n",
      "[17,   101] loss: 0.010\n",
      "[17,   121] loss: 0.010\n",
      "[17,   141] loss: 0.010\n",
      "[17,   161] loss: 0.010\n",
      "[17,   181] loss: 0.010\n",
      "[17,   201] loss: 0.010\n",
      "[17,   221] loss: 0.010\n",
      "[17,   241] loss: 0.010\n",
      "Finished Training\n",
      "[18,     1] loss: 0.000\n",
      "[18,    21] loss: 0.009\n",
      "[18,    41] loss: 0.009\n",
      "[18,    61] loss: 0.009\n",
      "[18,    81] loss: 0.009\n",
      "[18,   101] loss: 0.009\n",
      "[18,   121] loss: 0.010\n",
      "[18,   141] loss: 0.009\n",
      "[18,   161] loss: 0.010\n",
      "[18,   181] loss: 0.009\n",
      "[18,   201] loss: 0.009\n",
      "[18,   221] loss: 0.010\n",
      "[18,   241] loss: 0.010\n",
      "Finished Training\n",
      "[19,     1] loss: 0.000\n",
      "[19,    21] loss: 0.008\n",
      "[19,    41] loss: 0.008\n",
      "[19,    61] loss: 0.009\n",
      "[19,    81] loss: 0.009\n",
      "[19,   101] loss: 0.009\n",
      "[19,   121] loss: 0.010\n",
      "[19,   141] loss: 0.010\n",
      "[19,   161] loss: 0.008\n",
      "[19,   181] loss: 0.009\n",
      "[19,   201] loss: 0.009\n",
      "[19,   221] loss: 0.008\n",
      "[19,   241] loss: 0.009\n",
      "Finished Training\n",
      "[20,     1] loss: 0.000\n",
      "[20,    21] loss: 0.008\n",
      "[20,    41] loss: 0.010\n",
      "[20,    61] loss: 0.009\n",
      "[20,    81] loss: 0.009\n",
      "[20,   101] loss: 0.008\n",
      "[20,   121] loss: 0.008\n",
      "[20,   141] loss: 0.008\n",
      "[20,   161] loss: 0.008\n",
      "[20,   181] loss: 0.008\n",
      "[20,   201] loss: 0.009\n",
      "[20,   221] loss: 0.008\n",
      "[20,   241] loss: 0.008\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1aad59a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = GnadDataset(df_test)\n",
    "test_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_wrapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4a8a1e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_, l_, s_ = next(iter(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6476cce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_ = d_.to(device=device)\n",
    "l_ = l_.to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "39a4cf8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model(d_, s_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "13bd82b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 10])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c8ed38b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([2, 7, 8, 4, 2, 6, 2, 4, 4, 5, 8, 1, 8, 6, 0, 4, 4, 3, 4, 4, 2, 2, 1, 2,\n",
       "         8, 4, 2, 3, 2, 4, 7, 1], device='cuda:0'),\n",
       " tensor([2, 7, 8, 4, 4, 6, 4, 2, 1, 5, 8, 1, 8, 6, 0, 2, 7, 3, 4, 4, 2, 2, 1, 2,\n",
       "         8, 4, 2, 3, 2, 4, 7, 5], device='cuda:0'))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(prediction, dim=1), l_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3d29b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer(b).shape\n",
    "in_features_shape = torch.flatten(embedding_layer(b), start_dim=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e955be3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_layer = torch.nn.Linear(in_features_shape[1], 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c7c943",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = np.array(df_train[\"text\"])\n",
    "vocab = build_vocab_from_iterator(train_x, specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e90b24",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab99bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_vocab(vocab, \"vocabulary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787d674f",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = load_vocab(\"vocabulary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c08303",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = torch.tensor(a(df_train[\"text\"][0]), dtype=torch.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf8371c",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6302d32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2f0ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f44aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "F.pad(seq, (1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8776cc5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4ccfc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = GnadDataset(np.array(df_train[\"text\"]), np.array(df_train[\"label\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce3171b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(df_train[\"text\"]).dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f913f8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee63391",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle-cell-segmentation",
   "language": "python",
   "name": "kaggle-cell-segmentation"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
